include "application"

spark {
  master = ${?SPARK_HA_MASTER}
  cleaner.ttl = "5000"
  streaming.batch.interval = ${?SPARK_STREAMING_BATCH_INTERVAL}
}

cassandra {
  connection.host = "localhost"
  connection.rpc.post = ${?CASSANDRA_RPC_PORT}
  connection.native.port = ${?CASSANDRA_NATIVE_PORT}
  auth.username = ${?CASSANDRA_AUTH_USERNAME}
  auth.password = ${?CASSANDRA_AUTH_PASSWORD}
  connection.keep-alive = ${?CASSANDRA_KEEP_ALIVE_MS}
  connection.query.retry.count = ${?CASSANDRA_QUEUE_RETRY_COUNT}
  connection.reconnect-delay.min = ${?CASSANDRA_MIN_RECONNECT_DELAY_MS}
  connection.reconnect-delay.max = ${?CASSANDRA_MAX_RECONNECT_DELAY_MS}
  read.page.row.size = ${?CASSANDRA_READ_PAGE_ROW_SIZE}
  read.split.size = ${?CASSANDRA_READ_SPLIT_SIZE}
  read.consistency.level = ${?CASSANDRA_READ_CONSISTENCY_LEVEL}
  write.concurrent.writes = ${?CASSANDRA_WRITE_CONCURRENT_WRITES}
  write.batch.size.bytes = ${?CASSANDRA_WRITE_BATCH_SIZE_BYTES}
  write.batch.size.rows = ${?CASSANDRA_WRITE_BATCH_SIZE_ROWS}
  write.max-bytes = ${?CASSANDRA_WRITE_MAX_BYTES}
  write.consistency.level = ${?CASSANDRA_WRITE_CONSISTENCY_LEVEL}
}

akka {
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 60s
  log-dead-letters = off
  log-dead-letters-during-shutdown = off

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      port = 2550
      hostname = "127.0.0.1"
    }
  }

  actor = {
    provider = "akka.cluster.ClusterActorRefProvider"

    default-dispatcher {
      throughput = 10
    }
  }

  cluster {
    log-info = on
    seed-nodes = []
    roles = ["analytics"]
    gossip-interval = 5s
    publish-stats-interval = 10s
    auto-down-unreachable-after = 10s
    metrics.gossip-interval = 10s
    metrics.collect-interval = 10s
  }
}

kafka {
  hosts = "localhost:9092"
  ingest-rate = 1s

  zookeeper {
    connection = "localhost"
    port = 2181
  }
  group.id = "event_in"
  topic.raw = stats_pipeline
  partitioner.fqcn = "kafka.producer.DefaultPartitioner"
  encoder.fqcn = "kafka.serializer.StringEncoder"
  decoder.fqcn = "kafka.serializer.StringDecoder"
  batch.send.size = 100
}
